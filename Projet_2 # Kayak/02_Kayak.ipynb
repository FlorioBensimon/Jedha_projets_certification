{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the cities were in average on the 7 days the probability to have rain is the lowest\n",
    "\n",
    "dataset=pd.read_csv('villes - Projection meteo.csv')\n",
    "\n",
    "dataset_annexe = dataset.groupby('city_name')['prob_precipitation']\\\n",
    "                .mean()\\\n",
    "                .sort_values(ascending=True)\\\n",
    "                .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Montauban', 'Toulouse', 'Carcassonne']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_cities = dataset_annexe.iloc[0:3, :]['city_name']\n",
    "top_cities_name = list(top_cities)\n",
    "top_cities_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Scrapy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 09:41:25 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-09-20 09:41:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-5.4.109+-x86_64-with-glibc2.10\n",
      "2021-09-20 09:41:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-09-20 09:41:25 [scrapy.extensions.telnet] INFO: Telnet Password: 2b1397d41c393583\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-09-20 09:41:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-09-20 09:41:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-09-20 09:41:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-09-20 09:41:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-09-20 09:41:25 [scrapy.extensions.telnet] INFO: Telnet Password: 20c86329022cad07\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-09-20 09:41:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-09-20 09:41:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-09-20 09:41:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2021-09-20 09:41:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-09-20 09:41:25 [scrapy.extensions.telnet] INFO: Telnet Password: 26c6137d8e7c86fe\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-09-20 09:41:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-09-20 09:41:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-09-20 09:41:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-09-20 09:41:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025\n",
      "2021-09-20 09:41:25 [scrapy.core.engine] ERROR: Error while obtaining start requests\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/scrapy/core/engine.py\", line 129, in _next_request\n",
      "    request = next(slot.start_requests)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/scrapy/spiders/__init__.py\", line 77, in start_requests\n",
      "    yield Request(url, dont_filter=True)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/scrapy/http/request/__init__.py\", line 25, in __init__\n",
      "    self._set_url(url)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/scrapy/http/request/__init__.py\", line 63, in _set_url\n",
      "    raise TypeError(f'Request url must be str or unicode, got {type(url).__name__}')\n",
      "TypeError: Request url must be str or unicode, got list\n",
      "2021-09-20 09:41:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-09-20 09:41:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'elapsed_time_seconds': 0.058141,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 9, 20, 9, 41, 25, 794060),\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 28,\n",
      " 'memusage/max': 118841344,\n",
      " 'memusage/startup': 118841344,\n",
      " 'start_time': datetime.datetime(2021, 9, 20, 9, 41, 25, 735919)}\n",
      "2021-09-20 09:41:25 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2021-09-20 09:41:51 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-09-20 09:41:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-09-20 09:41:51 [scrapy.extensions.feedexport] INFO: Stored json feed (506 items) in: res/_hotel_booking.json\n",
      "2021-09-20 09:41:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 35962,\n",
      " 'downloader/request_count': 22,\n",
      " 'downloader/request_method_count/GET': 22,\n",
      " 'downloader/response_bytes': 3584530,\n",
      " 'downloader/response_count': 22,\n",
      " 'downloader/response_status_count/200': 22,\n",
      " 'elapsed_time_seconds': 25.507935,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 9, 20, 9, 41, 51, 262235),\n",
      " 'httpcompression/response_bytes': 20142305,\n",
      " 'httpcompression/response_count': 22,\n",
      " 'item_scraped_count': 506,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 24,\n",
      " 'memusage/max': 118841344,\n",
      " 'memusage/startup': 118841344,\n",
      " 'request_depth_max': 21,\n",
      " 'response_received_count': 22,\n",
      " 'scheduler/dequeued': 22,\n",
      " 'scheduler/dequeued/memory': 22,\n",
      " 'scheduler/enqueued': 22,\n",
      " 'scheduler/enqueued/memory': 22,\n",
      " 'start_time': datetime.datetime(2021, 9, 20, 9, 41, 25, 754300)}\n",
      "2021-09-20 09:41:51 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2021-09-20 09:42:15 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-09-20 09:42:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-09-20 09:42:15 [scrapy.extensions.feedexport] INFO: Stored json feed (1000 items) in: res/_hotel_booking.json\n",
      "2021-09-20 09:42:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 67985,\n",
      " 'downloader/request_count': 41,\n",
      " 'downloader/request_method_count/GET': 41,\n",
      " 'downloader/response_bytes': 6765949,\n",
      " 'downloader/response_count': 41,\n",
      " 'downloader/response_status_count/200': 41,\n",
      " 'elapsed_time_seconds': 49.707299,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 9, 20, 9, 42, 15, 490388),\n",
      " 'httpcompression/response_bytes': 39515898,\n",
      " 'httpcompression/response_count': 41,\n",
      " 'item_scraped_count': 1000,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 20,\n",
      " 'memusage/max': 119087104,\n",
      " 'memusage/startup': 119087104,\n",
      " 'request_depth_max': 40,\n",
      " 'response_received_count': 41,\n",
      " 'scheduler/dequeued': 41,\n",
      " 'scheduler/dequeued/memory': 41,\n",
      " 'scheduler/enqueued': 41,\n",
      " 'scheduler/enqueued/memory': 41,\n",
      " 'start_time': datetime.datetime(2021, 9, 20, 9, 41, 25, 783089)}\n",
      "2021-09-20 09:42:15 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "#Creation of the class BookingSpider(scrapy.Spider)\n",
    "\n",
    "destination_name = 'Carcassonne' #Name to be replaced with the city to be studied\n",
    "\n",
    "class BookingSpider1(scrapy.Spider):\n",
    "    name = \"hotels\"\n",
    "\n",
    "    start_urls = ['https://www.booking.com/index.fr.html', 'https://www.booking.com/...'],\n",
    "\n",
    "# Parse function for choosing the city\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to choose\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata = {'ss': destination_name[0]},\n",
    "            callback = self.after_search\n",
    "        )\n",
    "\n",
    "    def after_search(self, response):\n",
    "\n",
    "\n",
    "\n",
    "        for hotel in response.css('.sr_item'):\n",
    "            yield {\n",
    "                'city': destination_name[0],\n",
    "                'name': hotel.css('.sr-hotel__name::text').get(),\n",
    "                'url': \"https://www.booking.com\" + hotel.css('.sr-hotel__title a').attrib[\"href\"],\n",
    "                'gps': hotel.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                'score': hotel.css('.bui-review-score__badge::text').get(),\n",
    "                'description': hotel.css('.hotel_desc::text').get()\n",
    "\n",
    "            }\n",
    "\n",
    "        # Select the NEXT button and store it in next_page\n",
    "\n",
    "        try:\n",
    "                next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "                logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)\n",
    "\n",
    "class BookingSpider2(scrapy.Spider):\n",
    "        name = \"hotels\"\n",
    "\n",
    "        start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for choosing the city\n",
    "        def parse(self, response):\n",
    "            # FormRequest used to choose\n",
    "            return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata = {'ss': destination_name[1]},\n",
    "                callback = self.after_search\n",
    "            )\n",
    "\n",
    "        def after_search(self, response):\n",
    "\n",
    "\n",
    "\n",
    "            for hotel in response.css('.sr_item'):\n",
    "                yield {\n",
    "                    'city': destination_name[1],\n",
    "                    'name': hotel.css('.sr-hotel__name::text').get(),\n",
    "                    'url': \"https://www.booking.com\" + hotel.css('.sr-hotel__title a').attrib[\"href\"],\n",
    "                    'gps': hotel.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                    'score': hotel.css('.bui-review-score__badge::text').get(),\n",
    "                    'description': hotel.css('.hotel_desc::text').get()\n",
    "\n",
    "                }\n",
    "\n",
    "            # Select the NEXT button and store it in next_page\n",
    "\n",
    "            try:\n",
    "                    next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "            except KeyError:\n",
    "                    logging.info('No next page. Terminating crawling process.')\n",
    "            else:\n",
    "                yield response.follow(next_page, callback = self.after_search)\n",
    "\n",
    "class BookingSpider3(scrapy.Spider):\n",
    "        name = \"hotels\"\n",
    "\n",
    "        start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for choosing the city\n",
    "        def parse(self, response):\n",
    "            # FormRequest used to choose\n",
    "            return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata = {'ss': destination_name[2]},\n",
    "                callback = self.after_search\n",
    "            )\n",
    "\n",
    "        def after_search(self, response):\n",
    "\n",
    "\n",
    "\n",
    "            for hotel in response.css('.sr_item'):\n",
    "                yield {\n",
    "                    'city': destination_name[2],\n",
    "                    'name': hotel.css('.sr-hotel__name::text').get(),\n",
    "                    'url': \"https://www.booking.com\" + hotel.css('.sr-hotel__title a').attrib[\"href\"],\n",
    "                    'gps': hotel.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                    'score': hotel.css('.bui-review-score__badge::text').get(),\n",
    "                    'description': hotel.css('.hotel_desc::text').get()\n",
    "\n",
    "                }\n",
    "\n",
    "            # Select the NEXT button and store it in next_page\n",
    "\n",
    "            try:\n",
    "                    next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "            except KeyError:\n",
    "                    logging.info('No next page. Terminating crawling process.')\n",
    "            else:\n",
    "                yield response.follow(next_page, callback = self.after_search)\n",
    "               \n",
    "                \n",
    "# Name of the file where the results will be saved\n",
    "filename = \"_hotel_booking.json\"\n",
    "\n",
    "#If file already exists, delete it before crawling\n",
    "if filename in os.listdir('res/'):\n",
    "    os.remove('res/' + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)', #ne fonctionne pas avec Mozilla ?\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'res/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BookingSpider1)\n",
    "process.crawl(BookingSpider2)\n",
    "process.crawl(BookingSpider3)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>name</th>\n",
       "      <th>hotel_id</th>\n",
       "      <th>url</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city_name, city_id, name, hotel_id, url, latitude, longitude, score, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creation of Dataframe for the top 3 cities with details on hotels\n",
    "\n",
    "df_top_cities_hotels = pd.DataFrame(columns = [\"city_name\", \"city_id\", \"name\", \"hotel_id\", \"url\",\n",
    "                                        \"latitude\", \"longitude\", \"score\", \"description\"])\n",
    "df_top_cities_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7131509dbfda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Reading JSON files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"res/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_hotel_booking.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'hotel_id'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"city_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "for city in top_cities_name:\n",
    "    city_name = city\n",
    "    city_id = dataset.loc[dataset[\"city_name\"] == city,\"city_id\"].values[0]\n",
    "    \n",
    "    #Reading JSON files\n",
    "    path = \"res/\" + city +'_hotel_booking.json'\n",
    "    temp = pd.read_json(str(path))\n",
    "    temp = temp.reset_index().rename({'index': 'hotel_id'}, axis = 1)\n",
    "    temp.loc[:, \"city_name\"] = city_name\n",
    "    temp.loc[:, \"city_id\"] = city_id\n",
    "    \n",
    "    #Clean name, desc and link\n",
    "    temp[\"description\"] = temp[\"description\"].apply(lambda x : x.replace('\\n',''))\n",
    "    temp[\"name\"] = temp[\"name\"].apply(lambda x : x.replace('\\n',''))\n",
    "    temp[\"url\"] = temp[\"url\"].apply(lambda x : x.replace('\\n',''))\n",
    "    \n",
    "    #Transform score\n",
    "    temp[\"score\"] = temp[\"score\"].str.replace(',','.')\n",
    "    \n",
    "    \n",
    "    #Get the lat and lon out of GPS information\n",
    "    temp.loc[:, \"gps\"] = temp[\"gps\"].str.split(',')\n",
    "    temp.loc[:, \"latitude\"] = temp[\"gps\"].apply(lambda x : x[1])\n",
    "    temp.loc[:, \"longitude\"] = temp[\"gps\"].apply(lambda x : x[0])\n",
    "    \n",
    "    temp = temp.drop('gps', axis = 1)\n",
    "    temp = temp.drop('city', axis = 1)\n",
    "    \n",
    "    \n",
    "    df_top_cities_hotels = df_top_cities_hotels.append(temp)\n",
    "    \n",
    "df_top_cities_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change type  \n",
    "df_top_cities_hotels[\"latitude\"] = df_top_cities_hotels[\"latitude\"].astype(float)\n",
    "df_top_cities_hotels[\"longitude\"] = df_top_cities_hotels[\"longitude\"].astype(float)\n",
    "df_top_cities_hotels[\"score\"] = df_top_cities_hotels[\"score\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the weather information in a CSV file\n",
    "df_top_cities_hotels.to_csv('res/3 best cities - hotels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.8/site-packages (5.3.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from plotly) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#Creating map for the 3 best cities\n",
    "\n",
    "!pip install plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# setting Jedha color palette as default\n",
    "pio.templates[\"jedha\"] = go.layout.Template(\n",
    "    layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    ")\n",
    "pio.templates.default = \"jedha\"\n",
    "pio.renderers.default = \"iframe\" # to be replaced by \"iframe\" if working on JULIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>average_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carcassonne</td>\n",
       "      <td>28</td>\n",
       "      <td>43.213036</td>\n",
       "      <td>2.349107</td>\n",
       "      <td>22.712857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toulouse</td>\n",
       "      <td>30</td>\n",
       "      <td>43.604462</td>\n",
       "      <td>1.444247</td>\n",
       "      <td>21.441429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montauban</td>\n",
       "      <td>31</td>\n",
       "      <td>44.017584</td>\n",
       "      <td>1.354999</td>\n",
       "      <td>21.068571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     city_name  city_id   latitude  longitude  average_temperature\n",
       "0  Carcassonne       28  43.213036   2.349107            22.712857\n",
       "1     Toulouse       30  43.604462   1.444247            21.441429\n",
       "2    Montauban       31  44.017584   1.354999            21.068571"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataset with latitude and longitude infos for the 10 best cities\n",
    "\n",
    "df_top_cities = dataset.drop([\"day_month\", \"weather_desc\", \"prob_precipitation\", \"daily_temperature\"], axis=1)\n",
    "\n",
    "df_top_cities = df_top_cities.drop_duplicates()\n",
    "\n",
    "df_top_cities = df_top_cities.loc[df_top_cities[\"city_name\"]\\\n",
    "                                .isin(top_cities_name)]\\\n",
    "                                .reset_index()\\\n",
    "                                .drop(\"index\", axis = 1)\n",
    "df_top_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_13.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying a map of these 3 top cities with their average temperature\n",
    "\n",
    "fig = px.scatter_mapbox(df_top_cities, \n",
    "                        lat = \"latitude\", \n",
    "                        lon = \"longitude\",\n",
    "                        color = \"average_temperature\", \n",
    "                        hover_name = \"city_name\",\n",
    "                        mapbox_style = \"carto-positron\", \n",
    "                        size = \"average_temperature\",\n",
    "                        title = 'Average Temperatures in selected cities')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.write_html(fig, \"average_temperature_in_selected_cities.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   city_name    0 non-null      object \n",
      " 1   city_id      0 non-null      object \n",
      " 2   name         0 non-null      object \n",
      " 3   hotel_id     0 non-null      object \n",
      " 4   url          0 non-null      object \n",
      " 5   latitude     0 non-null      float64\n",
      " 6   longitude    0 non-null      float64\n",
      " 7   score        0 non-null      float64\n",
      " 8   description  0 non-null      object \n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_top_cities_hotels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>name</th>\n",
       "      <th>hotel_id</th>\n",
       "      <th>url</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city_name, city_id, name, hotel_id, url, latitude, longitude, score, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning the non-filled score\n",
    "df_top_cities_hotels_with_score = df_top_cities_hotels.sort_values(by=['score'], ascending=False)\n",
    "#df_top_cities_hotels_with_score = df_top_cities_hotels_with_score.iloc[0:10]\n",
    "\n",
    "df_top_cities_hotels_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_17.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Creating the map\n",
    "fig2 = px.scatter_mapbox(df_top_cities_hotels_with_score, \n",
    "                        lat = \"latitude\",\n",
    "                        lon = \"longitude\",\n",
    "                        color = \"score\",\n",
    "                        size = \"score\",\n",
    "                        hover_name = \"name\",\n",
    "                        mapbox_style = \"carto-positron\",\n",
    "                        title = 'Best hotels in top 3 cities')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.write_html(fig, \"best_hotels_in_top_3_cities.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
